{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have attempted to learn Natural Language Processing all by myself and decided to apply these techniques in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "string = '''I like teddy bears. I dont know why but teddy bears\n",
    "            give me comfort. They are also fluffy. Teddy bears are really\n",
    "            the best man! \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, as an example, I am using a string variable to ensure text is available for use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implentation of Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\H.Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, I thought of it as a way to explain the different techniques of tokenizing. Using "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I like teddy bears.',\n",
       " 'I dont know why but teddy bears\\n            give me comfort.',\n",
       " 'They are also fluffy.',\n",
       " 'Teddy bears are really\\n            the best man!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"sent_tokenize\" is one way to implement tokenization. As you can see, when i compiled the function, it took the string and parsed to where the sentences are seperated to elements of an array!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'like',\n",
       " 'teddy',\n",
       " 'bears',\n",
       " '.',\n",
       " 'I',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'why',\n",
       " 'but',\n",
       " 'teddy',\n",
       " 'bears',\n",
       " 'give',\n",
       " 'me',\n",
       " 'comfort',\n",
       " '.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'also',\n",
       " 'fluffy',\n",
       " '.',\n",
       " 'Teddy',\n",
       " 'bears',\n",
       " 'are',\n",
       " 'really',\n",
       " 'the',\n",
       " 'best',\n",
       " 'man',\n",
       " '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordtokens = word_tokenize(string)\n",
    "len(wordtokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"word_tokenize\" is another great function to implement as the words are seperated and created in elements of array. This is especially great for data scientists when certain keyboards are being researched.\n",
    "\n",
    "In total, we can see there are total of 14 elements in the array!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'teddy': 3, 'bears': 3, '.': 3, 'i': 2, 'are': 2, 'like': 1, 'dont': 1, 'know': 1, 'why': 1, 'but': 1, ...})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n",
    "\n",
    "for word in wordtokens:\n",
    "    fdist[word.lower()] +=1\n",
    "    \n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2265faa7f48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By looking at the graph, the highest frequent keywords that are found  are 'teddy' and 'bear'\n",
    "\n",
    "Teddy: 3 times,\n",
    "Bear: 3 times\n",
    "\n",
    "This is great for finding repeating keywords. It helps someone maybe to understand by seeing any relations in the text itself. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
